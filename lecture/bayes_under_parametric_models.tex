\documentclass[18pt, handout]{beamer}

\usetheme{metropolis}
\usefonttheme{professionalfonts}


%% "Patch" to keep the text widths' of Metropolis similar to that of Madrid.
%\setbeamersize{text margin left=15pt, text margin right=15pt}
%\makeatletter
%\setbeamertemplate{title page}{
%\centering
%  \begin{minipage}[b][\paperheight]{.9\textwidth}
%    \ifx\inserttitlegraphic\@empty\else\usebeamertemplate*{title graphic}\fi
%    \vfill%
%    \ifx\inserttitle\@empty\else\usebeamertemplate*{title}\fi
%    \ifx\insertsubtitle\@empty\else\usebeamertemplate*{subtitle}\fi
%    \usebeamertemplate*{title separator}
%    \ifx\beamer@shortauthor\@empty\else\usebeamertemplate*{author}\fi
%    \ifx\insertdate\@empty\else\usebeamertemplate*{date}\fi
%    \ifx\insertinstitute\@empty\else\usebeamertemplate*{institute}\fi
%    \vfill
%    \vspace*{1mm}
%  \end{minipage}
%}
%\makeatother

% Modify the text widths' of section title pages. See `beamerinnerthememetropolis.dtx`.
% `\paperheight` requires adjustment after modifying the frame content placement. Not sure why.
\makeatletter
\setbeamertemplate{section page}{
  \centering
  \begin{minipage}[c][.95\paperheight]{.9\textwidth}
    \raggedright
    \usebeamercolor[fg]{section title}
    \usebeamerfont{section title}
    \insertsectionhead\\[-1ex]
    \usebeamertemplate*{progress bar in section page}
    \par
    \ifx\insertsubsectionhead\@empty\else%
      \usebeamercolor[fg]{subsection title}%
      \usebeamerfont{subsection title}%
      \insertsubsectionhead
    \fi
  \end{minipage}
  \par
  \vspace{\baselineskip}
}
\makeatother

% Make frametitles larger and place closer to center. See `beamerinnerthememetropolis.dtx`.
\setbeamerfont{frametitle}{size=\Large}
\makeatletter
% Theme default adds paddings of 2.2ex to all the four sides of frame titles
\newlength{\metropolis@frametitle@toppadding}
\newlength{\metropolis@frametitle@bottompadding}
\setlength{\metropolis@frametitle@toppadding}{3.5ex} 
\setlength{\metropolis@frametitle@bottompadding}{0ex} 
\setlength{\metropolis@frametitle@padding}{4ex} % Horizontal padding to left
\renewcommand{\metropolis@frametitlestrut@start}{
  \rule{0pt}{\metropolis@frametitle@toppadding +%
    \totalheightof{%
      \ifcsdef{metropolis@frametitleformat}{\metropolis@frametitleformat X}{X}%
    }%
  }%
}
\newcommand*\getlength[1]{\number#1}
\renewcommand{\metropolis@frametitlestrut@end}{%
  \ifnum\getlength{\metropolis@frametitle@bottompadding}>0%
    \rule[-\metropolis@frametitle@bottompadding]{0pt}{\metropolis@frametitle@bottompadding}%
  \fi%
}
\makeatother

% Move up the frame content a bit. See:
% https://github.com/matze/mtheme/blob/master/source/beamerinnerthememetropolis.dtx#L509-L523
% https://tex.stackexchange.com/questions/247826/beamer-full-vertical-centering
\makeatletter
\define@key{beamerframe}{c}[true]{% centered
  \beamer@frametopskip=0pt plus 0.5fill\relax% Default is `1fill`
  \beamer@framebottomskip=0pt plus 1.5fill\relax%
  \beamer@frametopskipautobreak=0pt plus .4\paperheight\relax%
  \beamer@framebottomskipautobreak=0pt plus .6\paperheight\relax%
  \def\beamer@initfirstlineunskip{}%
}
\makeatother


%% Packages
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{mathtools} % For \coloneqq (among others)
\usepackage{subcaption}
\usepackage{bm}
\usepackage{accents}
\usepackage{pgfplots}
%\usepackage[normalem]{ulem}
%	\renewcommand{\ULthickness}{.6pt} % default is 0.4pt
%	\newcommand{\coloredUline}{\bgroup\markoverwith{\textcolor{lava}{\rule[-0.5ex]{2pt}{0.6pt}}}\ULon}
\usepackage{natbib}
	\bibliographystyle{abbrvnat}
	\setcitestyle{authoryear, open={(}, close={)}}
\usepackage{bibentry} % For \nobibliography

%% Beamer options.
\setbeamercovered{dynamic}
\setbeamercovered{invisible}
\beamertemplatenavigationsymbolsempty
%\setbeamertemplate{caption}{unnumbered}
\setbeamertemplate{footline}{}
%\setbeameroption{show notes on second screen}

%% Customize styles inside itemize environments
%\settowidth{\leftmargini}{\usebeamertemplate{itemize item}}
%\addtolength{\leftmargini}{-1.2\labelsep}
\setbeamerfont{itemize/enumerate subbody}{size=\normalsize} %to set the body size
\setbeamertemplate{itemize subitem}{\normalsize\raise1.25pt\hbox{\donotcoloroutermaths$\blacktriangleright$}}  % to set the symbol size

%% Custom environments
\newcommand{\defineTightItemizeSpacing}{%
	\setlength{\abovedisplayskip}{.25\baselineskip}%
	\setlength{\belowdisplayskip}{.25\baselineskip}%
}
\newenvironment{tightEquation}{%
	\defineTightItemizeSpacing%
	\begin{equation}
}{
	\end{equation} \ignorespacesafterend
}
\makeatletter
\newenvironment{tightEquation*}{%
	\defineTightItemizeSpacing%
	\begin{equation*}
}{
	\end{equation*} \ignorespacesafterend
}
\newcommand{\defineWithinItemizeSpacing}{%
	\setlength{\abovedisplayskip}{.3\baselineskip}%
	\setlength{\belowdisplayskip}{.3\baselineskip}%
}
\newenvironment{itemizedEquation}{%
	\defineWithinItemizeSpacing%
	\begin{equation}
}{
	\end{equation} \ignorespacesafterend
}
\makeatletter
\newenvironment{itemizedEquation*}{%
	\defineWithinItemizeSpacing%
	\begin{equation*}
}{
	\end{equation*} \ignorespacesafterend
}
\newenvironment{indented}[1][3]{%
	\hfill \begin{minipage}{\dimexpr\textwidth-#1ex} 
	}{
	\end{minipage}
}
\newenvironment{wideitemize}{%
  \begin{itemize}
  \addtolength\itemsep{.4\baselineskip}
}{
  \end{itemize}
}
\newenvironment{narrowItemize}[1][]{%
  \vspace{-.3\baselineskip}%
  \begin{itemize}[#1]
  \addtolength\itemsep{-.1\baselineskip}
}{
  \end{itemize}
}
\newenvironment{narrowEnumerate}[1][1.]{%
  \vspace{-.3\baselineskip}%
  \begin{enumerate}[#1]
  \addtolength\itemsep{-.1\baselineskip}
}{
  \end{enumerate}
}

%% Color definitions
\definecolor{turquoise}{rgb}{0.19, 0.84, 0.78}
\definecolor{mediumturquoise}{rgb}{0.28, 0.82, 0.8}
\definecolor{lava}{rgb}{0.81, 0.06, 0.13}
\colorlet{linkColor}{mediumturquoise}
\colorlet{highlightedTextColor}{lava}

%% Set the color theme of the presentation
\definecolor{jhuBlue}{RGB}{0, 45, 114} % "Heritage" blue; https://brand.jhu.edu/color/
\definecolor{jhuSpiritBlue}{RGB}{114, 172, 229} % lighter blue
\colorlet{themecolor}{jhuBlue}
\colorlet{bgcolor}{themecolor}
\colorlet{textcolor}{white}
\usecolortheme[named=themecolor]{structure}
%\setbeamercolor{titlelike}{fg=black}
\setbeamercolor{frametitle}{fg=themecolor, bg=white}
\setbeamercolor{section in head/foot}{fg=textcolor}
\setbeamercolor{background canvas}{bg=white}
%\colorlet{toccolor}{black}
%\setbeamercolor{section in toc}{fg=toccolor}
\setbeamercolor{block title}{bg=themecolor!75!gray!25}
\setbeamercolor{block body}{bg=themecolor!75!gray!5}


% Macros

%% Utility macros
\newcommand{\noteBullet}{\hspace*{.75em}\textcolor{themecolor}{$\blacktriangleright$}\ }

%% General math macros
\newcommand{\given}{\thinnerspace | \thinnerspace}
\newcommand{\divby}{\thinnerspace /}
\newcommand{\defeq}{\vcentcolon =} % `\coloneqq` and `\vcentcolon` requires the `mathtools` package: https://tex.stackexchange.com/questions/194344/symbol-for-definition
\newcommand{\diff}{\operatorname{\mathrm{d}}\!{}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\medcap}{\mathbin{\mathsmaller{\bigcap}}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\elemwiseProd}{\odot}
\renewcommand{\complement}{{\raisebox{1.5pt}{\scriptsize $\mathsf{c}$}}}
\newcommand{\transpose}{\text{\raisebox{.5ex}{$\intercal$}}}
\newcommand{\lowerscript}[1]{\raisebox{-2pt}{\scriptsize $#1$}}
\newcommand{\yesnumber}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\thinnerspace}{\mskip.5\thinmuskip}
\newcommand{\thinnestspace}{\mskip.25\thinmuskip}
\newcommand{\spaceBeforePartial}{\mskip\thinmuskip}
\newcommand{\scriptsumi}{{\scriptsize \sum_i}} % Suscript needs to be within bracket, hence the hard coding

%%% Define a larger "hat" of fixed width: https://tex.stackexchange.com/questions/20473/how-can-i-manually-choose-the-size-of-a-wide-accent-math-mode/20477#20477
\DeclareMathSymbol{\widehatsym}{\mathord}{largesymbols}{"62}
\newcommand{\lowerwidehatsym}{\text{\smash{\raisebox{-1.3ex}{$\widehatsym$}}}}
\newcommand\fixedwidehat[1]{%
  \mathchoice
    {\accentset{\displaystyle\lowerwidehatsym}{#1}}
    {\accentset{\textstyle\lowerwidehatsym}{#1}}
    {\accentset{\scriptstyle\lowerwidehatsym}{#1}}
    {\accentset{\scriptscriptstyle\lowerwidehatsym}{#1}}
}

%% Probability / statistics macros
\newcommand{\probability}{\mathbb{P}}
\newcommand{\expectation}{\mathbb{E}}
\newcommand{\variance}{\mathrm{Var}}
\DeclareMathOperator*{\covariance}{Cov}
\newcommand{\indicator}{\mathds{1}}
\newcommand{\eqDistribution}{\mathrel{\raisebox{-.2ex}{$\overset{\scalebox{.6}{$\, d$}}{=}$}}}
\newcommand{\iidSim}{\mathrel{\raisebox{-.3ex}{$\overset{\text{i.i.d.}}{\sim}$}}}
\newcommand{\unifDist}{\mathrm{Unif}}
\newcommand{\normalDist}{\mathcal{N}}
\newcommand{\betaDist}{\mathrm{Beta}}
\newcommand{\gammaDist}{\mathrm{Gamma}}
\newcommand{\mle}[1]{\widehat{#1}_{\textrm{mle}}}
\newcommand{\map}[1]{\widehat{#1}_{\textrm{map}}}
\newcommand{\empiricalVar}{\widehat{\variance}}
\newcommand{\kldivergence}{D_{\mathrm{KL}}}
\newcommand{\infoMat}{\mathcal{I}}

%% Variable macros / Aliases
\newcommand{\density}{\pi}
\newcommand{\likelihood}{L}
\newcommand{\by}{\bm{y}}
\newcommand{\bX}{\bm{X}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\btheta}{\bm{\theta}}


\title{\centerline{Bayesian analysis under parametric models}}
\author{%
	Aki Nishimura\\
	Department of Biostatistics%
}
%\institute[]{}
\date{}

\titlegraphic{%
	\begin{picture}(0,0)
		% Try coordinate (155, -150) for a one-line title, (165, -160) if two-lines, and (155, -172) if three-lines
		\put(165, -160){\makebox(0,0)[lb]{\includegraphics[width=.45\textwidth]{jhsph_biostat_logo}}}
	\end{picture}%
}

\begin{document}

\maketitle

%\section{Bayesian estimation of incidence proportion}

\begin{frame}
\frametitle{Example: estimating incidence proportion}
\textbf{Question:} 
\only<+>{``Does the new vaccine cause \ldots?''}%
\only<+->{``Does the new vaccine cause Guillain-Barré syndrome?''}
% Note: Guillain-Barré --- rare but potentially a life-threatening disorder with body's immune system attacking nerves and causing muscle weakness

\begin{minipage}[t][2\baselineskip]{\linewidth}
\only<+>{%
(Unfortunately, this turned out to be the case for the 1976 swine flu vaccine, albeit the syndrome remaining extremely rare.)
}%
\only<+->{%
To answer, we want to estimate the incidence proportion within, say, 4 weeks of vaccination and compare it to the background rate.
}%
\end{minipage}

\pause[\thebeamerpauses]
\textbf{Model:} We assume a binomial model
\begin{tightEquation*}
y_1, \ldots, y_n \given p 
	\iidSim \mathrm{Bernoulli}(p)
\end{tightEquation*}
with the likelihood function
\begin{tightEquation*}
\likelihood(\by \given p) 
	= \textstyle \prod_i p^{y_i} (1 - p)^{1 - y_i}
	= p^{{\scriptscriptstyle \sum_i} y_i} (1 - p)^{n - {\scriptscriptstyle \sum_i} y_i}.
\end{tightEquation*}

\pause
\textbf{Inference:}
Maximizing the likelihood gives us $\mle{p} = {\scriptstyle \sum_i} y_i \divby n$.

\pause 
Problem? 
There's a good chance $\mle{p} = 0$; the null incidence of Guillain-Barré is $\leq 1.5$ in million during 4-week period.

\pause
Potential fix is continuity correction, which yields $\widehat{p} = 0.5 \divby (n + 1)$.
\end{frame}


\begin{frame}
\frametitle{Example: estimating incidence proportion}

Let's try Bayesian analysis of the binomial model with likelihood
\begin{tightEquation*}
\likelihood(\by \given p) 
	= p^{{\scriptscriptstyle \sum_i} y_i} (1 - p)^{n - {\scriptscriptstyle \sum_i} y_i}.
\end{tightEquation*}

\pause
\textbf{Prior:} 
Convenient, reasonably flexible prior here is the beta dist:
\begin{tightEquation*}
\density(p) \propto p^a (1 - p)^b \sim \mathrm{Beta}(a, b)
\end{tightEquation*}
with $\expectation[p] = a \divby (a + b)$ and $\variance[p] = \expectation[p] (1 - \expectation[p]) \divby (a + b + 1)$.

\pause
\vspace*{.25\baselineskip}
\textbf{Posterior:} 
Bayes' rule tells us that
\begin{tightEquation*}
\begin{aligned}
\density(p \given \by)
%	\propto \likelihood(\by \given p) \density(p)
%	&\propto p^{a + \scriptscriptsum_i y_i} (1 - p)^{b + n - \scriptscriptsum_i y_i} \\
	&\sim \mathrm{Beta}(a + {\scriptstyle \sum_i} y_i, b + n - {\scriptstyle \sum_i} y_i).
\end{aligned}
\end{tightEquation*}

\pause
In particular, the posterior mean of $p$ is
\begin{tightEquation*}
\widehat{p}_{\textrm{bayes}} 
	= \expectation[p \given y] 
	= \frac{a + {\scriptstyle \sum_i} y_i,}{a + b + n}
	= \frac{(a + b) \thinnerspace \expectation[p] + n \thinnerspace \mle{p},}{a + b + n},
\end{tightEquation*}
\pause
where $a + b$ can be interpreted as \textit{prior sample size}.
% Note: the similarity to MLE. In particular, the posterior mean coincides with MLE in the limit $a, b \to 0$. 
\end{frame}

\begin{frame}
\frametitle{Example: estimating incidence proportion}

Posterior provides much more information than a point estimate. 

\pause
For example, we can calculate a posterior 95\% credible interval by solving for $l$ and $u$ such that
\begin{tightEquation*}
\int_l^u \density(p \given \by) \diff p = 0.95,
\end{tightEquation*}
\pause%
as well as a post predictive dist on future data $\by_{\textrm{new}}$ via relation
\begin{tightEquation*}
\density(\by_{\textrm{new}} \given \by)
	= \int \likelihood(\by_{\textrm{new}} \given p) \thinnerspace \density(p \given \by) \diff p.
\end{tightEquation*}

\pause
\textbf{Note:} Bayesian credible intervals, philosophically speaking, have completely different interpretations from frequentist conf intervals.
\end{frame}

\begin{frame}
\frametitle{Example: estimating incidence proportion}

The remaining question is: how do we choose the prior?

\pause
Bayesians can, for better or worse, incorporate existing scientific knowledge into a prior.

\pause
To properly construct such an informed prior, we have to:
\pause%
\begin{narrowItemize}[<+->]
\item Find every post-vaccine incidence proportion of Guillain-Barré ever reported (and unreported?) in the literature, 
\item Conduct some sort of meta-analysis to obtain a distribution $\density(p)$ for a ``generic'' vaccine, 
\item Somehow account for the difference between mRNA and conventional vaccines, 
%\item Wonder how to incorporate other (e.g.\ pharmacologic) evidence, 
\item Wonder how many other bloody things you have to consider,
\item And maybe start to despair at some point?
\end{narrowItemize}
\end{frame}

\begin{frame}
\frametitle{Example: estimating incidence proportion}

As a more pragmatic alternative to constructing a scientifically informed prior, we can consider specifying it in terms of: 
\vspace*{-.1\baselineskip}
\begin{narrowItemize}
\item Prior mean $a \divby (a + b)$,
\item Prior sample size $a + b$, and/or
\item Prior credible intervals.
\end{narrowItemize}
\vspace*{-.1\baselineskip}

\pause
For example, we can 
\vspace*{-.1\baselineskip}
\begin{narrowItemize}
\item Fix the prior sample size to be $1$, and
\item Use a reported background rate of Guillain-Barré ($\approx 1$ per million over 4-weeks) as the prior mean.
\end{narrowItemize}
\vspace*{-.4\baselineskip}
\pause
In other words, solve $(a + b) = 1$ and $a \divby (a + b) = 10^{-6}$ for $a$ and $b$. % which gives $a = 10^{-6}$ and $b = 1 - a$.

\pause
Or, we can choose $a$ and $b$ so that the prior 95\% CI spans the background rate and highest vaccine-induced rate ever reported.%

\end{frame}


\begin{frame}
\frametitle{Example: estimating incidence proportion}

It's also possible to avoid relying on existing evidence by using an \textit{objective prior};
e.g.\ $\betaDist(1/2, 1/2)$ for the binomial model:%

\pause%
\begin{narrowItemize}
\item Removes subjective decisions in Bayesian inference; 
\pause
\item Justified in particular by its frequentist property: Bayesian CI $[q_{\alpha}, 1]$ and $[0, q_{1 - \alpha}]$ achieve coverage at level $1 - \alpha + O(1/n)$;
\pause
\item Excellent frequentist property in non-asymptotic regimes.
\end{narrowItemize}

\pause
The corresponding post mean coincides with the freq estimator based on continuity correction:
\begin{tightEquation*}
\widehat{p}_{\textrm{bayes}} 
	= \expectation[p \given y] 
	= \frac{{\scriptstyle \sum_i} y_i + 1/2}{n + 1}.
\end{tightEquation*}
\end{frame}

\begin{frame}
\frametitle{Example: estimating incidence proportion}

The prior $\propto p^{-1/2} (1 - p)^{-1/2}$ may not be the most obvious choice.
\onslide<2->{But the more obvious ``flat'' prior has its own problems.}

\vspace{.15\baselineskip}
\begin{minipage}{.48\linewidth}
	\begin{figure}
		\begin{tikzpicture}
		\begin{axis}[
			width=0.575\paperheight,
			height=0.575\paperheight,
		    axis lines = left,
		    xmin=0,
		    xmax=1,
		    ymin=0,
		    ymax=3.5,
		    xlabel = \(p\),
		    ylabel = {\(\)},
		    legend style={
		    	font=\small, 
		    	draw=none, % Remove legend box
		    	empty legend % Remove line marker
		    }
		]
		%Below the red parabola is defined
		\addplot [
			domain=0.01:0.99,
		    samples=100, 
		    color=jhuBlue,
		    thin
		]
		{x^(-1/2) * (1 - x)^(-1/2) / pi};
		\addlegendentry{\({\tiny \betaDist\!\left(\thinnerspace p \thinnerspace \middle| \thinnerspace \frac12, \frac12\right)}\)}
		\end{axis}
		\end{tikzpicture}
	\end{figure}
\end{minipage}
~
\onslide<2->{%
\begin{minipage}{.48\linewidth}
	\begin{figure}
		\begin{tikzpicture}
		\begin{axis}[
			width=0.575\paperheight,
			height=0.575\paperheight,
		    axis lines = left,
		    xmin=0,
		    xmax=1,
		    ymin=0,
		    ymax=3.5,
		    xlabel = \(p\),
		    ylabel = {\(\)},
		    legend style={
		    	font=\small, 
		    	draw=none, % Remove legend box
		    	empty legend % Remove line marker
		    }
		]
		%Below the red parabola is defined
		\addplot [
			domain=0.01:0.99,
		    samples=100, 
		    color=jhuSpiritBlue,
		    thin
		]{1};
		\addlegendentry{\({\tiny \betaDist\!\left(\thinnerspace p \thinnerspace \middle| \thinnerspace 1, 1\right)}\)}
		\end{axis}
		\end{tikzpicture}
	\end{figure}
\end{minipage}
}%
\vspace{.15\baselineskip}
\onslide<3>{%
At the least, Bayesian inference has an explicit, clear interpretation in terms of the prior.
}
\end{frame}

\section{\centerline{Gaussian likelihood models}}


\begin{frame}
\frametitle{Univariate case}

Consider the problem of estimating a population mean $\mu$ under
\begin{tightEquation*}
y_1, \ldots, y_n \given \mu, \sigma \iidSim \normalDist(\mu, \sigma^2).
\end{tightEquation*}

For simplicity, let's first consider $\sigma$ to be fixed.
Also, for convenience, we parametrize in terms of \textit{precision} $\phi = \sigma^{-2}$.
\end{frame}

\newcommand{\priorPrec}{\phi_0}
\newcommand{\postPrec}{\phi_{\textrm{post}}}
\begin{frame}
\frametitle{Univariate case}
Convenient prior on $\mu$ is Gaussian:\ under the prior $\mu \sim \normalDist(\mu_0, \priorPrec^{-1})$,%
\begin{tightEquation*}
\begin{aligned}
\density(\mu \given \bm{y}, \phi)
	&\propto \likelihood(\bm{y} \given \mu, \phi ) \density(\mu) \\
%	&\propto \exp\!\left( - \frac{\phi}{2} \sum_i (y_i - \mu)^2\right)
%		\exp\!\left( - \frac{\priorPrec}{2} (\mu - \mu_0)^2 \right) \\
	&\propto \phi^{n / 2} \exp\!\left( - \frac12 \left[
			\phi \sum_i (y_i - \mu)^2
			+ \priorPrec (\mu - \mu_0)^2
		\right] \right) \\
	&\propto \exp\!\left( - \frac12 \left[
			\phi \left( n \mu^2 - 2 \mu {\textstyle \sum_i} y_i \right)
			+ \priorPrec (\mu^2 - 2 \mu \mu_0)
		\right] \right) \\
	&= \exp\!\left( - \frac12 \left[
			(n \phi + \priorPrec) \mu^2 
			- 2 \mu \big( \phi {\textstyle \sum_i} y_i + \priorPrec \mu_0 \big)
		\right] \right) \\
	&= \exp\!\left( - \frac12 \left[
			\postPrec \left(
				\mu^2 - 2 \mu \mu_{\textrm{post}} 
			\right) 
		\right] \right) 
\end{aligned}
\end{tightEquation*}
for $\postPrec = n \phi + \priorPrec$ and $\mu_{\textrm{post}} = \postPrec^{-1} \left( \phi {\textstyle \sum_i} y_i + \priorPrec \mu_0 \right)$.

\vspace*{.3\baselineskip}
In other words, the posterior is:
$
\mu \given \bm{y}, \phi
	\sim \normalDist\!\left( \mu_{\textrm{post}}, \postPrec \right).
$
\end{frame}

\begin{frame}
\frametitle{Univariate case}
Note that, as in the beta-binomial case, the posterior mean is a weighted average of prior mean and {\small MLE}:
\begin{tightEquation*}
\mu_{\textrm{post}} = \frac{1}{n \phi + \priorPrec} \left( n \phi \mle{\mu} + \priorPrec \mu_0 \right)
\ \text{ where } \, \mle{\mu} = {\textstyle \sum_i} y_i / n.
\end{tightEquation*}

Objective ``prior'' for this model is a flat dist $\density(\mu) \propto 1$, which can be though of as the limit $\priorPrec \to 0$ of diminishing prior info. 
%\hspace*{.5em}\textcolor{themecolor}{$\blacktriangleright$} 

The flat prior is an example of \textit{improper priors}, whose use is justified by the posterior being a proper distribution.

Under the flat prior, the posterior becomes 
$\mu \given \bm{y}, \phi \sim \normalDist\!\left( n \phi, \mle{\mu} \right)$,
inferences under which coincides with frequentist ones.

\textbf{Note:} The flat prior (and $\betaDist(1/2, 1/2)$) also has a formal justification as being a Jeffreys/reference prior.
\end{frame}

\begin{frame}
\frametitle{Univariate case: side note on conjugate priors}
We've seen that a Gaussian (beta) prior leads to a Gaussian (beta) posterior under the Gaussian (binomial) likelihood model.

\smallskip
More generally, a prior is said to be \textit{conjugate} for the likelihood if the corresponding posterior belongs to the same distribution family.

\smallskip
Conjugate priors lead to (semi) analytically tractable posteriors and%
\begin{narrowItemize}
\item play an important, if not as critical as in the pre-Stan era, role in computationally tractable Bayesian modeling;
\item provide insights into the mechanics of Bayesian inference.
\end{narrowItemize}

\end{frame}

\begin{frame}
\frametitle{Univariate case with unknown variance}
Easy enough to infer by examining the likehood
\begin{tightEquation*}
\likelihood(\bm{y} \given \mu, \phi )
	\propto \phi^{n/2} \exp\!\left( - \frac12 
			\phi \sum_i (y_i - \mu)^2
		\right)
\end{tightEquation*}
that a (conditionally) conjugate prior for $\phi$ is a gamma distribution:
\begin{tightEquation*}
\density(\phi) 
	\propto \phi^{a - 1} \exp(-b x)
	\sim \gammaDist(\textrm{shape} = a, \thinnerspace \textrm{rate} = b)
\end{tightEquation*}
with $\expectation[\phi] = a / b$ and $\variance[\phi] = a / b^2$.

\smallskip
The corresponding posterior is
\begin{tightEquation*}
\phi \given \bm{y}, \mu 
	\sim \gammaDist\!\left(a + \frac{n}{2}, \, b + \frac{1}{2} \textstyle \sum_i (y_i - \mu)^2 \right).
\end{tightEquation*}
\end{frame}

\begin{frame}
\frametitle{Univariate case with unknown variance}
For interpretability, more helpful to parametrize the prior as 
\begin{tightEquation*}
\phi \sim \gammaDist\!\left( \frac{\nu_0}{2}, \thinnerspace \frac{\nu_0 \varphi_0^{-1}}{2} \right),
%\gammaDist\!\left( \textrm{shape} = \frac{\nu_0 \varphi_0}{2}, \thinnerspace \textrm{rate} = \frac{\nu_0}{2} \right)
\end{tightEquation*} 
under which $\expectation[\phi] = \varphi_0$ and $\variance[\phi] = 2 \varphi_0 / \nu_0$.

\medskip
The posterior now takes the form
\begin{tightEquation*}
\phi \given \bm{y}, \mu 
	\sim \gammaDist\!\left(
		\frac{\nu_0 + n}{2}, \, % \frac{1}{2} (\nu_0 + n), \, 
		\frac{\nu_0 \varphi_0^{-1} + \textstyle \sum_i (y_i - \mu)^2}{2} % \frac{1}{2} \left(\nu_0 \varphi_0^{-1} + \textstyle \sum_i (y_i - \mu)^2 \right) 
	\right)
\end{tightEquation*}
with
\begin{tightEquation*}
\expectation\left[ \phi \given \bm{y}, \mu \right]^{-1}
	= \frac{1}{\nu_0 + n} \left( \nu_0 \varphi_0^{-1} + n \thinnerspace \frac{\textstyle \sum_i (y_i - \mu)^2}{n} \right).
\end{tightEquation*}

\smallskip
In particular, $\nu_0$ can be interpreted as a prior degrees of freedom.
\end{frame}

\begin{frame}
\frametitle{Univariate case with unknown variance}
Objective prior $\density(\phi) \propto 1 / \phi$ coincides with the limit $\nu_0 \to 0$ and yields the posterior
\begin{tightEquation*}
\phi \given \bm{y}, \mu 
	\sim \gammaDist\!\left(
		\frac{n}{2}, \,
		\frac{\textstyle \sum_i (y_i - \mu)^2}{2} % \frac{1}{2} \left(\nu_0 \varphi_0^{-1} + \textstyle \sum_i (y_i - \mu)^2 \right) 
	\right)
\end{tightEquation*}
with
\begin{tightEquation*}
\expectation\left[ \phi \given \bm{y}, \mu \right]^{-1}
	= \frac{\textstyle \sum_i (y_i - \mu)^2}{n}
	= 1 \divby \mle{\phi}.
\end{tightEquation*}

Posterior becomes dominated by likelihood as prior becomes more diffuse, so the agreement with {\small MLE} isn't entirely surprising but \ldots

it's also worth noting the posterior mean differs from the mode
\begin{tightEquation*}
{\textstyle \argmax_{\phi}} \density\!\left( \phi \given \bm{y}, \mu \right)
	= \max\!\left\{
		0, \, \frac{n - 2}{\textstyle \sum_i (y_i - \mu)^2}
	\right\}.
\end{tightEquation*}
\end{frame}


\begin{frame}
\frametitle{Univariate case with unknown variance}
We now turn to the problem of estimating $\mu$ and $\phi$ simultaneously.

Most obvious choice is to assume $\mu$ and $\phi$ are a priori independent:
\begin{tightEquation*}
\density(\mu, \phi) = 
	\normalDist\!\left( \mu \given \mu_0, \priorPrec \right)
	\times
	\gammaDist\!\left( \phi \given \nu_0 / 2, \thinnerspace \nu_0 \varphi_0^{-1} / 2 \right).
\end{tightEquation*}

This prior, however, is only conditionally conjugate; i.e.\ 
while $\mu \given \bm{y}, \phi$ and $\phi \given \bm{y}, \mu$ belong to the original families, $\mu, \phi \given \bm{y}$ does not.

\textbf{Note:} The independent prior is otherwise a perfectly good choice!

Conjugate priors are worthy additions to your toolbox when they exist, but there is no reason to be religious about them.
\end{frame}


\begin{frame}
\frametitle{Univariate case with unknown variance}
To identify a conjugate prior, we let $\bar{y} = n^{-1} \sum_i y_i$ and observe 
\begin{tightEquation*}
\begin{aligned}
\likelihood(\bm{y} \given \mu, \phi )
	&\propto \phi^{n/2} \exp\!\left( - \frac12 
			\phi \sum_i (y_i - \mu)^2
		\right) \\
	&\propto \phi^{n/2} \exp\!\left( - \frac12 
			\phi \sum_i (y_i - \bar{y})^2
		\right)
		\exp\!\left( - \frac12 
			n \phi (\mu - \bar{y})^2
		\right)
\end{aligned}
\end{tightEquation*}
b/c $\sum_i (y_i - \mu)^2 = \sum_i (y_i - \bar{y})^2 + \sum_i (\bar{y} - \mu)^2$.

\medskip
After staring at the last expression long enough, you'd realize that
\begin{tightEquation*}
\likelihood(\bm{y} \given \mu, \phi )
	\propto 
		\gammaDist\!\left( \phi \, \middle| \,  \frac{n + 1}{2}, \, \frac{\sum_i (y_i - \bar{y})^2}{2} \right)  
		\normalDist\!\left( \mu \, \middle| \, \bar{y}, \, \frac{1}{n \phi} \right).
\end{tightEquation*}
% Note: the "-1" in $n - 1$ comes from the need for $\phi^{n/2}$ for the Gaussian density.
\end{frame}


\begin{frame}
\frametitle{Univariate case with unknown variance}
It's now easy enough to guess the following prior as conjugate:
\begin{tightEquation*}
\phi \sim \gammaDist\!\left( \frac{\nu_0}{2}, \thinnerspace \frac{\nu_0 \varphi_0^{-1}}{2} \right), \
\mu \given \phi \sim \normalDist\!\left( \mu_0, \thinnerspace \frac{1}{\kappa_0 \phi} \right)
\vspace*{-.15\baselineskip}% 
\end{tightEquation*}
with density 
\vspace*{-.15\baselineskip}
\begin{tightEquation*}
\density(\mu, \phi)
	\propto \phi^{\frac{\nu_0}{2} - 1} \exp\!\left( - \frac{\nu_0 \varphi_0^{-1}}{2} \phi \right)
		\exp\!\left( - \frac{1}{2} \kappa_0 \phi (\mu - \mu_0)^2 \right).
\end{tightEquation*}

\medskip
The corresponding posterior is given by (homework)
\begin{tightEquation*}
\begin{aligned}
&\phi \given \bm{y} 
	\sim \gammaDist\!\left(
		\frac{\nu_0 + n}{2}, \thinnerspace b_{\textrm{post}}%
	\right), \\
&\mu \given \bm{y}, \phi 
	\sim \normalDist\!\left( \frac{\kappa_0 \mu_0 + n \bar{y}}{\kappa_0 + n}, \thinnerspace \frac{1}{(\kappa_0 + n) \phi} \right),
\end{aligned}
\end{tightEquation*}
where 
$\, b_{\textrm{post}} = {\displaystyle \frac12} \left(
	\nu_0 \varphi_0^{-1} 
	+ \textstyle \sum_i (y_i - \bar{y})^2
	+ \frac{\kappa_0 n}{\kappa_0 + n} (\mu_0 - \bar{y} )^2
\right)$.
\end{frame}


\begin{frame}
\frametitle{Univariate case with unknown variance}
The posterior estimates mostly have the same structure as before, but let's take a look at
\begin{equation*}
\expectation[ \phi \given \bm{y} ]
	= \frac{1}{\nu_0 + n} \left(
		\nu_0 \varphi_0^{-1} 
		+ \textstyle \sum_i (y_i - \bar{y})^2
		+ \frac{\kappa_0 n}{\kappa_0 + n} (\mu_0 - \bar{y} )^2
	\right).
\end{equation*}

The $\frac{\kappa_0 n}{\kappa_0 + n} (\mu_0 - \bar{y} )^2$ part shows one way in which $\mu$ and $\phi$ interact:
\begin{tightEquation*}
\text{%
	discrepancy between $\mu_0$ and $\bar{y}$ % Note: $\mu_0$ = prior guess and $\bar{y}$ = empirical mean 
	$\, \Rightarrow \,$ inflated estimate of $\phi$.
}%
\end{tightEquation*}

\smallskip
But note also this ``bias'' diminishes as $n \to \infty$ since $\frac{\kappa_0 n}{\kappa_0 + n} \leq \kappa_0$.
\end{frame}


\begin{frame}
\frametitle{Univariate case with unknown variance}
The prior has a feature that the apriori (conditional) variance $\variance(\mu \given \phi) = 1 / \kappa_0 \phi$ depends on the noise variance $1 / \phi$.

\smallskip
The prior thus has the behavior ``smaller the $\phi$ and noisier the $y_i$'s, less informative the prior on $\mu \given \phi$.''

\smallskip
This may not be an accurate reflection of the prior knowledge, but is a reasonable data-adaptive behavior.
\end{frame}


\begin{frame}
\frametitle{Univariate case with unknown variance}
Identifying an appropriate objective prior isn't always straight- forward in multi-parameter settings, but the established one here is:
\begin{tightEquation*}
\density(\mu, \phi) \propto 1 / \phi.
\end{tightEquation*}

The objective prior yields a posterior
\begin{tightEquation*}
%\begin{aligned}
\phi \given \bm{y} 
	\sim \gammaDist\!\left(
		\frac{n - 1}{2},
		\frac{\textstyle \sum_i (y_i - \bar{y})^2}{2} 
	\right), \
\mu \given \bm{y}, \phi 
	\sim \normalDist\!\left( \bar{y}, \thinnerspace \frac{1}{n \phi} \right)
%\end{aligned}
\end{tightEquation*}
with
\begin{tightEquation*}
\expectation\!\left[ 
	(\mu, \phi) \given \bm{y}
\right]
	= \left( 
		\bar{y}, \, \frac{n - 1}{\textstyle \sum_i (y_i - \bar{y})^2}
	\right).
\end{tightEquation*}
\end{frame}


\section{Linear regression}

\newcommand{\bx}{\bm{x}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bPhi}{\bm{\Phi}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\minus}{\scalebox{1.0}[1.0]{{\scriptsize -}}}
\newcommand{\nPred}{p}
\begin{frame}
\frametitle{Linear regression}
Consider a regression model with design matrix $\bX \in \mathbb{R}^{n \times \nPred}$
\begin{tightEquation*}
\bm{y} = \bX \bbeta + \bm{\epsilon} 
	\ \text{ for } \,
	\bm{\epsilon} \sim \normalDist\!\left(\bm{0}, \phi^{-1} \bm{I} \right);
\end{tightEquation*}
or, to be more precise, 
\begin{tightEquation*}
\bm{y} \given \bX, \bbeta, \phi
	\sim \normalDist\!\left( \bX \bbeta, \phi^{- 1} \bm{I} \right).
\end{tightEquation*}

As we will see, posterior inference under this linear model is structurally very similar to the univariate Gaussian model.
\end{frame}

\begin{frame}
\frametitle{TBD}
The linear model has the likelihood
\begin{tightEquation*}
\begin{aligned}
\likelihood(\bm{y} \given \bX, \bbeta, \phi)
	&\propto \phi^{n / 2} \exp\!\left(
		- \frac{\phi}{2} \left\| \bm{y} - \bX \bbeta \right\|^2 
	\right) \\
	&\propto \phi^{n / 2} \exp\!\left(
		- \frac{1}{2} \left( 
			\phi \thinnerspace \bbeta^\transpose \bX^\transpose \bX \bbeta
			- 2 \thinnerspace \phi \thinnerspace \bbeta^\transpose \bX^\transpose \bm{y}
		\right) 
	\right).
\end{aligned}
\end{tightEquation*}

As a function of $\bbeta$, the likelihood looks like a Gaussian density
\begin{tightEquation*}
\normalDist\!\left( 
	\thinnerspace \bbeta \, \thinnerspace \middle| \thinnerspace
	\left( \bX^\transpose \bX \right)^{-1} \bX^\transpose \bm{y},
	\left( \phi \thinnerspace \bX^\transpose \bX \right)^{-1} 
\right).
\end{tightEquation*}

As a function of $\phi$,  the likelihood looks like a Gamma density.
%\begin{tightEquation*}
%\gammaDist\!\left(
%	\thinnerspace \phi \, \thinnerspace \middle| \thinnerspace
%	\frac{n}{2} + 1, 
%	\frac{\left\| \bm{y} - \bX \bbeta \right\|^2}{2}
%\right).
%\end{tightEquation*}

\smallskip
So you can guess the following prior to be conditionally conjugate:
\begin{tightEquation*}
\bbeta \sim \normalDist(\bmu_0, \bPhi_0^{-1})
\ \text{ and } \
\phi \sim \gammaDist\!\left( \frac{\nu_0}{2}, \thinnerspace \frac{\nu_0 \varphi_0^{-1}}{2} \right).
\end{tightEquation*}
\end{frame}

\begin{frame}
\frametitle{TBD}
In particular, note that $\bbeta \sim \normalDist(\bmu_0, \bPhi_0)$ means
\begin{tightEquation*}
\begin{aligned}
\density(\bbeta)
%	&\propto \exp\!\left(
%		- \frac{1}{2} (\bbeta - \bmu_0)^\transpose \bPhi_0 (\bbeta - \bmu_0)
%	\right) \\
	&\propto \exp\!\left(
		- \frac{1}{2} \left( 
			\bbeta^\transpose \bPhi_0 \bbeta 
			- 2 \bbeta^\transpose \bPhi_0 \bmu_0
		\right) 
	\right).
\end{aligned}
\end{tightEquation*}
So we have
\begin{tightEquation*}
\begin{aligned}
\density(\bbeta \given \bm{y}, \bX, \phi)
	&\propto \likelihood(\bm{y} \given \bbeta, \bX, \phi) \thinnerspace \density(\bbeta) \\
	&\propto \exp\!\left(
		- \frac{1}{2} \left( 
			\phi \thinnerspace \bbeta^\transpose \bX^\transpose \bX \bbeta
			- 2 \thinnerspace \phi \thinnerspace \bbeta^\transpose \bX^\transpose \bm{y}
		\right)
	\right) \\
	&\hphantom{\propto}\ \exp\!\left(
		- \frac{1}{2} \left( 
			\bbeta^\transpose \bPhi_0 \bbeta 
			- 2 \bbeta^\transpose \bPhi_0 \bmu_0
		\right) 
	\right) \\
	&\propto \exp\!\left(
		- \frac{\phi}{2} \left( 
			\bbeta^\transpose \bPhi_{\textrm{post}} \bbeta
			- 2 \bbeta^\transpose \bPhi_{\textrm{post}} \bmu_{\textrm{post}}
		\right) 
	\right),
\end{aligned}
\end{tightEquation*}
where $\bPhi_{\textrm{post}} = \phi \thinnerspace \bX^\transpose \bX + \bPhi_0$ and $\bmu_{\textrm{post}} = \bPhi_{\textrm{post}}^{-1} \left( \phi \thinnerspace \bX^\transpose \bm{y} + \bPhi_0 \bmu_0 \right)$.
\end{frame}

\begin{frame}
\frametitle{TBD}

In other words, the posterior conditional for $\bbeta$ is:
\begin{tightEquation*}
\bbeta \given \bm{y}, \bX, \phi
	\sim \normalDist\!\left(
		 \bmu_{\textrm{post}},
		 \bPhi_{\textrm{post}}
	\right)
\end{tightEquation*}
for $\bmu_{\textrm{post}} = \bPhi_{\textrm{post}}^{-1} \left( \phi \thinnerspace \bX^\transpose \bm{y} + \bPhi_0 \bmu_0 \right)$ and $\bPhi_{\textrm{post}} = \phi \thinnerspace \bX^\transpose \bX + \bPhi_0$.

\medskip
We similarly obtain the posterior conditional for $\phi$:
\begin{tightEquation*}
\phi \given \bm{y}, \bX, \bbeta 
	\sim \gammaDist\!\left( 
		\frac{\nu_0 + n}{2}, \thinnerspace \frac{\nu_0 \varphi_0^{-1} + \| \bm{y} - \bX \bbeta \|^2}{2} 
	\right).
\end{tightEquation*}

As in the univariate case, we could also consider a conjugate prior
\begin{tightEquation*}
\bbeta \given \phi \sim \normalDist(\bmu_0, (\kappa_0 \phi)^{-1} \bm{I})
\ \text{ and } \
\phi \sim \gammaDist\!\left( 
		\frac{\nu_0}{2}, \thinnerspace \frac{\nu_0 \varphi_0^{-1}}{2} 
	\right).
\end{tightEquation*}
% Homework idea: derive the corresponding posterior.
% Homework idea: derive the posterior under non iid noise.
\end{frame}


\begin{frame}
\frametitle{TBD}
Standard objective prior here is, unsurprisingly, $\thinnerspace \density(\bbeta, \phi) \propto 1 / \phi$.

The corresponding posterior is
\begin{tightEquation*}
\begin{gathered}
\bbeta \given \bm{y}, \bX, \phi
	\sim \normalDist\!\left(
		 \left(  \bX^\transpose \bX \right)^{-1} \! \bX^\transpose \bm{y}, \,
		\left( \phi \bX^\transpose \bX \right)^{-1}
	\right), \\
\phi \given \bm{y}, \bX
	\sim \gammaDist\!\left( 
		\frac{n - \nPred}{2}, \thinnerspace \frac{\| \bm{y} - \bX \mle{\bbeta} \|^2}{2} 
	\right). 
	% Note: $\mle{\bbeta} = \left( \bX^\transpose \bX \right)^{-1} \! \bX^\transpose \bm{y}$
\end{gathered}
\end{tightEquation*}

\textbf{Note:} The posterior is proper only if $n > \nPred$:
\begin{narrowItemize}
\item Flat prior in $\mathbb{R}^\nPred$ is even ``more improper/diffuse'' than in $\mathbb{R}$ because prior uncertainties in the parameters add up.
\item More generally, using improper or proper-but-diffuse priors is a risky business in high-dimensional (i.e.\ large $\nPred$) applications.
\item Requirement $n > \nPred$ is intuitive enough here but it's less obvious in more complex models (where Bayes really shines).
\end{narrowItemize}
\end{frame}


\begin{frame}
\frametitle{TBD}
As an illustration, let's consider constructing a proper yet arguably conservative prior for the linear model.

(``Conservative'' means biasing, if any, the inference towards the null and encoding less prior information than we actually have.)

To help us achieve this goal, let's first pre-process the data:
\begin{narrowEnumerate}
  \item Scale $\bm{y}$ and cont.\ $\bx_j\mkern-1mu$'s so that $\widehat{\variance}(\by) = 1$ and $\widehat{\variance}(\bx_j \mkern-2mu) = 1/2$.%
%  \item For each binary $\bx_j$, set the more freq category as its baseline; % (i.e. assigning it the value of $0$)
%  this ensures $x_{ij} = 0$ to better represent an average data point.
  \item Center $\bm{y}$ and all (both cont.\ and binary) $\bx_j$'s.
\end{narrowEnumerate}

\textbf{Note:} $\widehat{\variance}(\bx) = 1/2$ follows \citet{gelman2008default_prior} and is an attempt to put cont.\ and binary predictors on the same scale. 
\end{frame}


\begin{frame}
\frametitle{TBD}
With the pre-processed data $(\by, \bX)$, we can argue $| \beta_j | = 2$ to represent a reasonable upper bound on the effect sizes because:
\begin{tightEquation*}
| \beta_j | = 2
	\ \Rightarrow \ \widehat{\variance}(\beta_j \bx_j) = \widehat{\variance}(\by).
	% Note: In other words, the variation in the $
\end{tightEquation*}
In other words, the variation in $\bx_j$ explains most of that in $\by$. 

The prior choice $\beta_j \sim \normalDist(0, 1)$ hence seems reasonable. % with most of its probability in the range $[-2, 2]$.

\smallskip
As for the intercept, it seems reasonable to set $\beta_0 \sim \normalDist(0, 1)$ so that its prior variance matches that of $\by$.

\end{frame}

\begin{frame}
\frametitle{TBD}
For the residual variance $\phi^{-1}$, we can expect $\phi^{-1} \lesssim 1 = \widehat{\variance}(\by)$.

If we were to do without conjugacy, we might choose a prior like 
\begin{tightEquation*}
\sigma = \phi^{-1/2} \sim \normalDist^+(0, 1) 
	\ \text{ or } \, \unifDist(0, 2).
	% Note: the prior variance of 1 / 4 would favor \sigma < 1, which isn't ideal since \sigma may well be very close to 1. 
\end{tightEquation*}
(Unless there is a reason to assume a lower bound $\phi^{-1} \geq c > 0$.)

\smallskip
Expressing the same kind of prior assumption is difficult with the gamma prior on $\phi$.
	% Note: In particular, there is no way to make the density look flat near $\phi^{-1/2} \approx 0$.
	
But, based the interpretation of $\nu_0$ as the prior degrees of freedom,
\begin{tightEquation*}
\phi \sim \gammaDist(\nu_0 / 2, \nu_0 / 2)
	\ \text{ for } \, \nu_0 \in [0.1, \thinnerspace 1]
\end{tightEquation*}
with $\expectation[\phi] = 1 = \widehat{\variance}(\by)$ is defensible.
% Note: BUGS uses $\nu_0 = 0.002$.

\smallskip
\textbf{Note:} 
Typically, as long as the prior is in a reasonable range, the inference isn't too sensitive.
So don't fret about gory details.
% Note: Plus, 1) you can always do a sensitivity analysis; and 2) if the inference is sensitive, most likely it means you don't have enough data to avoid informative priors.
\end{frame}


\begin{frame}
\frametitle{TBD}
Incidentally, scaling $\by$ and $\bx_j$'s is equivalent to scaling the priors.

\smallskip
In fact, had we only centered them w/o scaling, the same inference would be obtained under the following prior:
\begin{tightEquation*}
\begin{gathered}
\beta_0 \sim \normalDist\!\left( 0, \empiricalVar(\by)^{1/2} \right), \ \,
\beta_j \sim\normalDist\!\left( 0, \, \frac{ \empiricalVar(\by)^{1/2} }{ \empiricalVar(\bx_j)^{1/2} } \right) \\
\phi \sim \gammaDist\!\left(
	\frac{\nu_0}{2}, 
	\frac{\nu_0}{2} \thinnerspace \empiricalVar(\by)^{-1}
\right).
%\beta_j \sim\normalDist\!\left( 0, \, \empiricalVar(\by)^{1/2} \big/ \, \empiricalVar(\bx_j)^{1/2} \right) \\
\end{gathered}
\end{tightEquation*}

\smallskip
Centering variables interacts with the inference as well; e.g.\ had we decided to center $\bx_j$'s but not $\by$, the equivalent inference requires
\begin{tightEquation*}
\beta_0 \sim \normalDist\!\left( \bar{y}, \thinnerspace \empiricalVar(\by)^{1/2} \right).
\end{tightEquation*}
\end{frame}


\section{\centerline{Asymptotic properties of Bayesian inference}}

\begin{frame}
\frametitle{Posteriors' asymptotic behavior}
We've seen examples in which posteriors yield comparable/ identical inferences as established frequentist procedures.

In fact, under regular parametric models, Bayesian inference is \textit{guaranteed} to coincide with a frequentist one asymptotically.

In a sense, this is to be expected from the fact 
\begin{tightEquation*}
\log \density(\btheta \given \by^n)
	= \mathrm{const} + \log \density(\btheta) + \textstyle \sum_{i = 1}^n \log \likelihood(y_i \given \btheta);
\end{tightEquation*}
where the sum of the $n$ i.i.d.\ terms should eventually dominate.

Here we will study some asymptotic behaviors of posteriors.
\end{frame}

\newcommand{\truthSub}{\mathrm{tru}}
\begin{frame}
\frametitle{Measuring ``distance'' between prob densities}
\textit{Kullback–Leibler divergence} measure how far away a probability density $p(\cdot)$ is from the reference $q(\cdot)$ and is defined as
\begin{tightEquation*}
\kldivergence( p \| q)
	\defeq - \int \log\!\left( \frac{q(x)}{p(x)} \right) p(x) \diff x
	= \expectation_{X \thinnestspace \sim \, p(\cdot)}\!\left[ \thinnerspace \log\!\left( \frac{q(X)}{p(X)} \right) \right].
\end{tightEquation*}
\noteBullet To see relevance, think $p(\cdot) = \likelihood(\thinnerspace \cdot \given \btheta)$ and $q(\cdot) = \likelihood(\thinnerspace \cdot \given \btheta_\truthSub)$.

\smallskip
In particular, the KL divergence has the property 
\begin{tightEquation*}
\kldivergence(p \| q) \geq 0
	\ \text{ and } \
	\kldivergence(p \| q) = 0 \, \text{ only if } \, p(\cdot) \equiv q(\cdot).
\end{tightEquation*}
%$\kldivergence(p \| q) \geq 0$ and $\kldivergence(p \| q) = 0$ only if $p(\cdot) \equiv q(\cdot)$. 

\textit{Proof:} 
Jensen's inequality says $f(\expectation[X]) \leq \expectation[f(X)]$ for convex $f$, with the equality only if $f(x) = x$ on the support of $\density_X(\cdot)$. Hence,
\begin{tightEquation*}
0 = - \log\!\left( \int q(x) \diff x \right)
	= - \log\!\left( \int \frac{q(x)}{p(x)} p(x) \diff x \right)
	\leq \kldivergence(p \| q).
\end{tightEquation*}
\end{frame}


\begin{frame}
\frametitle{Posterior concentration around true parameter}

We now show that posteriors necessarily concentrate around the true parameter $\btheta_\truthSub$ as $n \to \infty$. 
To see this, we observe that
\smallskip
\begin{tightEquation*}
\begin{aligned}
\log\!\left( 
	\frac{\density(\btheta_\truthSub \given \by^n)}{\density(\btheta \given \by^n)}
\right) % Note: while not explicitly denoted, $\by$ obviously depends on the sample size.
	&= \log\!\left( 
		\frac{%
			\density(\btheta_\truthSub) \prod_i \likelihood(y_i \given \btheta_\truthSub)
		}{%
			\density(\btheta) \prod_i \likelihood(y_i \given \btheta)
		}%
	\right) \\
	&= \log\!\left( 
		\frac{%
			\density(\btheta_\truthSub)
		}{%
			\density(\btheta)
		}%
	\right)
	+ \sum_{i = 1}^n \log\!\left( 
		\frac{%
			\likelihood(y_i \given \btheta_\truthSub)
		}{%
			\likelihood(y_i \given \btheta)
		}%
	\right).
\end{aligned}
\end{tightEquation*}
\smallskip
Now we only need to recognize the latter term as a KL divergence:
\smallskip
\begin{tightEquation*}
\frac{1}{n} \sum_i \log\!\left( 
	\frac{%
		\likelihood(y_i \given \btheta_\truthSub)
	}{%
		\likelihood(y_i \given \btheta)
	}%
\right) 
	\to - \int \log\!\left( 
		\frac{%
			\likelihood(y \given \btheta_\truthSub)
		}{%
			\likelihood(y \given \btheta)
		}%
	\right) \likelihood(y \given \btheta_\truthSub) \diff y.
\end{tightEquation*}

Thus
$\displaystyle 
	\frac{\density(\btheta_\truthSub \given \by^n)}{\density(\btheta \given \by^n)}
\to \infty$ whenever $\btheta \neq \btheta_\truthSub$;
i.e.\ $\density(\thinnerspace \cdot \given \by^n) \to \delta_{\btheta_\truthSub}(\cdot)$.
\end{frame}

\begin{frame}
\frametitle{Posterior concentration under mis-specified model}
Examining the proof, we see an analogous asymptotic result holds even when a model is \textit{mis-specified} --- 
the family $\{ \likelihood(\thinnerspace \cdot \given \btheta) \}_{\btheta}$ doesn't include the true data distribution $y \sim \likelihood_\truthSub(\cdot)$; e.g.\
\begin{narrowItemize}
\item $\operatorname{Skewness}(y) \neq 0$, but $\likelihood(\thinnerspace \cdot \given \mu, \phi) \sim \normalDist(\mu, \phi^{-1})$.
\item $\expectation[y] < 0$, but assumed $\mu \geq 0$ under $\likelihood(\thinnerspace \cdot \given \mu, \phi) \sim \normalDist(\mu, \phi^{-1})$.
\item $y = f(\bx) + \sigma(\bx) \epsilon$ for non-linear $f$ and non-const $\sigma$, \\ but $\likelihood(\thinnerspace \cdot \given \bx, \bbeta, \phi) \sim \normalDist(\bx^\transpose \bbeta, \phi^{-1})$.
\end{narrowItemize}

Under mis-specification, we can still speak of an optimal approx
\begin{tightEquation*}
\btheta_{\mathrm{opt}}
	= \textstyle \argmin_{\btheta} \kldivergence\!\left(
		\likelihood(\thinnerspace \cdot \given \btheta) \, \| \, \likelihood_\truthSub
	\right),
\end{tightEquation*}
\mbox{and similarly show $\displaystyle \frac{\density(\btheta_{\mathrm{opt}} \given \by^n)}{\density(\btheta \given \by^n)}
\to \infty$ if $\btheta_{\mathrm{opt}}\!$ is the unique minimizer.}
\end{frame}


\begin{frame}
\frametitle{Asymptotic Gaussianity of posteriors}
Besides the posterior concentration, we in fact have a Bayesian asymptotic theory analogous to the frequentist one for {\small MLE}'s:

\begin{theorem}[Bernstein-von-Mises/Bayesian Central Limit]
Under an appropriate regularity assumption, \\
\smallskip%
\makebox[\linewidth]{
$\density(\thinnerspace \cdot \given \by^n) 
	\to \normalDist\!\left(\btheta_\truthSub, \left[
		 n \infoMat(\btheta_\truthSub)
\right]^{-1} \right)$ \ as $\, n \to \infty$
}\\ % Note: This can be more precisely stated as the convergence of $n^{-1/2} (\Theta - \btheta_\truthSub)$.
\smallskip%
where 
$\, \infoMat(\btheta) = \expectation_{\thinnerspace y \thinnerspace \sim \thinnerspace  \likelihood(\thinnerspace \cdot \given \btheta)\!}\!\left[ 
	- \frac{\partial^2}{\partial \btheta^2} \log \likelihood(y \given \btheta)
\right]$.
\end{theorem}

\textbf{Cf.} Asymptotic theory for {\small MLE}: 
$\, \mle{\btheta} \to \normalDist(\btheta_\truthSub, \left[
		 n \infoMat(\btheta_\truthSub)
\right]^{-1})$.

\mbox{In particular, Bayes and freq CI's necessarily coincide asymptotically}
(even though their interpretations are completely distinct).
\end{frame}


\newcommand{\samplesizeDependentMap}{\widehat{\theta}^{\text{\raisebox{.1ex}{$\thinnerspace n$}}}}
\begin{frame}
\frametitle{Asymptotic Gaussianity: Proof sketch (univariate)}
Consider the Taylor expansion of $\log \density(\theta \given \by^n)$ around its mode $\samplesizeDependentMap$:
\begin{tightEquation*}
\begin{aligned}
\log \density(\theta \given \by^n)
	= \log \density\big( \samplesizeDependentMap \given \by^n \big)
	&+ \frac12 \big( \theta - \samplesizeDependentMap \big)^2 \frac{\diff^2}{\diff \theta^2} \log \density\big( \samplesizeDependentMap \given \by^n \big) \\
	&\hspace*{-1.5em}+ \frac{1}{3!} \big( \theta - \samplesizeDependentMap \big)^3 \frac{\diff^3}{\diff \theta^3} \log \density\big( \samplesizeDependentMap \given \by^n \big) 
		+ \ldots.
\end{aligned}
\end{tightEquation*}
Observe that 
\begin{tightEquation*}
\begin{aligned}
\frac1n \frac{\diff^k}{\diff \theta^k} \log \density\big( \theta \given \by^n \big) 
	&= \frac1n \frac{\diff^k}{\diff \theta^k} \log \density(\theta) 
		+ \frac1n \sum_{i = 1}^n \frac{\diff^k}{\diff \theta^k} \log \likelihood(y_i \given \theta) \\
	&\to \expectation_{\thinnerspace y \thinnerspace \sim \thinnerspace  \likelihood(\thinnerspace \cdot \given \theta_\truthSub)\!} \!\left[
		\frac{\diff^k}{\diff \theta^k} \log \likelihood(y \given \theta)
	\right] \, \text{ as } \thinnerspace n \to \infty.
\end{aligned}
\end{tightEquation*}
Since the posterior concentrates around $\samplesizeDependentMap \approx \theta_\truthSub$, the terms $\big( \theta - \samplesizeDependentMap \big)^3 \frac{\diff^3}{\diff \theta^3} ...$ becomes immaterial relative to $n \big( \theta - \samplesizeDependentMap \big)^2$. 
\hfill \qedsymbol
\end{frame}

\begin{frame}
\frametitle{``Counterexample'' to the asymptotic theory}
In invoking the law of large numbers in the proof, the fixed dimen- sionality of $\btheta$ and i.i.d.\ nature of $y_i$'s were important assumptions.

B{\small UT} these assumptions are violated by many modern statistical \mbox{methods; e.g.\ random effects, model selection, non-parametrics, etc.}

\mbox{(In fact, you should go Bayes only when classical methods fall short.)}
\end{frame}


\begin{frame}
\frametitle{``Counterexample'' to the asymptotic theory}
One counterexample comes from a simple random effects model
\begin{tightEquation*}
y_{i1}, y_{i2} \given \mu_i, \phi
	\sim \normalDist(\mu_i, \phi^{-1}), \ \,
\mu_i \sim \normalDist(\mu_{\textrm{grp}}, \phi_{\textrm{grp}}^{-1})
\end{tightEquation*}
for $i = 1, \ldots, n$ with ``across-group'' mean $\mu_{\textrm{grp}}$ and variance $\phi_{\textrm{grp}}^{-1}$.
% Note: For simplicity, we take the across-group mean and variance to be fixed.

(This is basically the model behind the Neyman-Scott paradox except for slight extension to incorporate shrinkage toward $\mu_{\textrm{grp}}$.)

\smallskip
For example, $y_{i1}$ and $y_{i2}$ could represent batting averages of the $i$-th player in his 1st and 2nd games (\'{a} la \citealt{efron1975stein_est}).

\smallskip
\textbf{Question:} Would the posterior  of $(\bmu, \phi)$ look Gaussian for $n$ large?
\end{frame}


\begin{frame}
\frametitle{``Counterexample'' to the asymptotic theory}
For simplicity, let's just assume the objective prior $\density(\bmu, \phi) \propto 1 / \phi$ \\
(i.e.\ $\phi_{\textrm{grp}} = \infty$), ignoring the shrinkage estimation aspect.

Asymptotic behavior, if real, shouldn't depend on a prior anyways.

we take $\mu_{\textrm{grp}}$ and $\phi_{\textrm{grp}}$ to be fixed and $\thinnerspace \density(\phi) \propto 1 / \phi$.


The corresponding posterior is
\begin{tightEquation*}
\begin{gathered}
\bbeta \given \bm{y}, \bX, \phi
	\sim \normalDist\!\left(
		 \left(  \bX^\transpose \bX \right)^{-1} \! \bX^\transpose \bm{y}, \,
		\left( \phi \bX^\transpose \bX \right)^{-1}
	\right), \\
\phi \given \bm{y}, \bX
	\sim \gammaDist\!\left( 
		\frac{n - \nPred}{2}, \thinnerspace \frac{\| \bm{y} - \bX \bbeta \|^2}{2} 
	\right).
\end{gathered}
\end{tightEquation*}
\end{frame}

\nobibliography{references} % To cite inline without creating a bibliography

\end{document}
